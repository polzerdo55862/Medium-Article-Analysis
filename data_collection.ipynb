{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "This notebook collects the data from different data sources and end points and saves the raw data in a SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following I search for articles published in a certain time frame in the TowardsDataScience archive (https://towardsdatascience.com/archive/year/month/day) \n",
    "\n",
    "References: \n",
    "https://hackernoon.com/how-to-scrape-a-medium-publication-a-python-tutorial-for-beginners-o8u3t69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# function to extract all the information to the stories given in the medium archive\n",
    "def extract_data(url, date_published):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "    stories_data = []\n",
    "    \n",
    "    # find attributes author_url, reading_time, reading_time, responses, story_url for the stories which where published \n",
    "    # for the specified date\n",
    "    \n",
    "    for story in stories:\n",
    "        each_story = []\n",
    "\n",
    "        author_box = story.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "        author_url = author_box.find('a')['href']\n",
    "\n",
    "        try:\n",
    "            reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        title = story.find('h3').text if story.find('h3') else '-'\n",
    "        subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "\n",
    "        if story.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents'):\n",
    "\n",
    "            claps = story.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents').text\n",
    "\n",
    "        else:\n",
    "            claps = 0\n",
    "\n",
    "        if story.find('a', class_='button button--chromeless u-baseColor--buttonNormal'):\n",
    "\n",
    "            responses = story.find('a', class_='button button--chromeless u-baseColor--buttonNormal').text\n",
    "\n",
    "        else:\n",
    "            responses = '0 responses'\n",
    "\n",
    "        story_url = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')['href']\n",
    "   \n",
    "        # data cleaning\n",
    "        reading_time = reading_time.split()[0]\n",
    "        responses = responses.split()[0]\n",
    "\n",
    "        story_page = requests.get(story_url)\n",
    "        story_soup = BeautifulSoup(story_page.text, 'html.parser')\n",
    "\n",
    "        sections = story_soup.find_all('section')\n",
    "        story_paragraphs = []\n",
    "        section_titles = []\n",
    "\n",
    "        for section in sections:\n",
    "            paragraphs = section.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                story_paragraphs.append(paragraph.text)\n",
    "\n",
    "            subs = section.find_all('h1')\n",
    "            for sub in subs:\n",
    "                section_titles.append(sub.text)\n",
    "\n",
    "        number_sections = len(section_titles)\n",
    "        number_paragraphs = len(story_paragraphs)\n",
    "\n",
    "        each_story.append(date_published)\n",
    "        each_story.append(title)\n",
    "        each_story.append(subtitle)\n",
    "        each_story.append(claps)\n",
    "        each_story.append(responses)\n",
    "        each_story.append(author_url)\n",
    "        each_story.append(story_url)\n",
    "        each_story.append(reading_time)\n",
    "        each_story.append(number_sections)\n",
    "        each_story.append(section_titles)\n",
    "        each_story.append(number_paragraphs)\n",
    "        each_story.append(story_paragraphs)\n",
    "\n",
    "        stories_data.append(each_story)\n",
    "\n",
    "    # write data to data frame \"df\" and return df\n",
    "    columns = ['date_published', 'title', 'subtitle', 'claps', 'responses', \n",
    "           'author_url', 'story_url', 'reading_time', \n",
    "           'number_sections', 'section_titles', \n",
    "           'number_paragraphs', 'paragraphs']\n",
    "\n",
    "    df = pd.DataFrame(stories_data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://towardsdatascience.com/making-python-programs-blazingly-fast-c1cd79bd1b32?source=collection_archive---------0-----------------------\n",
      "https://towardsdatascience.com/implementing-a-fully-convolutional-network-fcn-in-tensorflow-2-3c46fb61de3b?source=collection_archive---------1-----------------------\n",
      "https://towardsdatascience.com/6-new-features-in-python-3-8-for-python-newbies-dc2e7b804acc?source=collection_archive---------2-----------------------\n",
      "https://towardsdatascience.com/how-to-be-fancy-with-python-8e4c53f47789?source=collection_archive---------3-----------------------\n",
      "https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5?source=collection_archive---------4-----------------------\n",
      "https://towardsdatascience.com/from-scratch-to-search-playing-with-your-data-elasticsearch-ingest-pipelines-6d054bf5d866?source=collection_archive---------5-----------------------\n",
      "https://towardsdatascience.com/gan-pix2pix-generative-model-c9bf5d691bac?source=collection_archive---------6-----------------------\n",
      "https://towardsdatascience.com/an-introduction-to-decision-trees-with-python-and-scikit-learn-1a5ba6fc204f?source=collection_archive---------7-----------------------\n",
      "https://towardsdatascience.com/decision-trees-for-dummies-37dbee6c7169?source=collection_archive---------8-----------------------\n",
      "https://towardsdatascience.com/kaggle-user-survey-2019-326e187ff207?source=collection_archive---------9-----------------------\n",
      "https://towardsdatascience.com/find-linear-transformation-based-on-known-points-64785e987db0?source=collection_archive---------10-----------------------\n",
      "https://towardsdatascience.com/visualizing-marginal-effects-using-ggeffects-in-r-4e7fb0569040?source=collection_archive---------11-----------------------\n",
      "https://towardsdatascience.com/the-decade-of-data-science-4c247aaafa2f?source=collection_archive---------12-----------------------\n",
      "https://towardsdatascience.com/smallest-neural-network-for-complete-beginners-in-4-mins-5a398b574053?source=collection_archive---------13-----------------------\n",
      "https://towardsdatascience.com/do-data-science-good-until-do-it-great-in-2020-7466598c5cd0?source=collection_archive---------14-----------------------\n",
      "https://towardsdatascience.com/building-my-neural-network-from-scratch-cbb97321cb8f?source=collection_archive---------15-----------------------\n",
      "https://towardsdatascience.com/can-the-dynamic-linking-of-neural-activations-bring-us-closer-to-strong-ai-bc85b64c9f82?source=collection_archive---------16-----------------------\n",
      "https://towardsdatascience.com/inference-of-origin-destination-flows-and-activity-locations-81f56fbe7c45?source=collection_archive---------17-----------------------\n",
      "https://towardsdatascience.com/3-ways-risk-management-methods-can-be-misleading-and-how-to-fix-it-c9b60ae0f535?source=collection_archive---------18-----------------------\n",
      "https://towardsdatascience.com/beware-of-implementing-machine-learning-models-as-black-box-tools-d08947db7536?source=collection_archive---------19-----------------------\n",
      "https://towardsdatascience.com/the-power-of-mathematical-ingenuity-49c7b6cfe05e?source=collection_archive---------20-----------------------\n",
      "https://towardsdatascience.com/finding-social-behavior-patterns-through-call-detail-records-e3c107be62f7?source=collection_archive---------21-----------------------\n",
      "https://towardsdatascience.com/guinea-pig-breed-classification-517fbb036ee6?source=collection_archive---------22-----------------------\n",
      "https://towardsdatascience.com/build-a-text-classification-model-using-watson-autoai-3650c461642?source=collection_archive---------23-----------------------\n",
      "https://towardsdatascience.com/my-capstone-project-exploring-toronto-and-searching-for-the-best-place-to-establish-an-indian-af140e77250b?source=collection_archive---------24-----------------------\n",
      "https://towardsdatascience.com/big-tech-is-coming-for-us-but-we-can-stop-it-a61ba00101e9?source=collection_archive---------25-----------------------\n",
      "https://towardsdatascience.com/trigraph-how-to-use-graphs-to-analyse-triathlon-events-145c5daefcc5?source=collection_archive---------26-----------------------\n",
      "https://towardsdatascience.com/the-good-bad-and-ugly-fantasy-football-trades-i-made-in-2019-253fa0124629?source=collection_archive---------27-----------------------\n"
     ]
    }
   ],
   "source": [
    "url = f'https://towardsdatascience.com/archive/2020/01/01'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "for story in stories:\n",
    "    story_url = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')['href']\n",
    "    print(story_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results in a SQLite database\n",
    "After each scraping process, save the content of the data frame into a SQLite database. You can find the functions used in the module helper_functions.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import helper_functions\n",
    "\n",
    "# connect to SQLite database medium.db\n",
    "con = helper_functions.create_sqlite_connection(\"medium.db\")\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sql statement to create table \"stories\" if not exist\n",
    "sql_create_table_stories = \"\"\"\n",
    "                                CREATE TABLE IF NOT EXISTS test (\n",
    "                                \n",
    "                                    date_published INTEGER PRIMARY KEY,\n",
    "                                    title TEXT NOT NULL,\n",
    "                                    subtitle TEXT,\n",
    "                                    claps TEXT NOT NULL,\n",
    "                                    responses INTEGER,\n",
    "                                    author_url TEXT NOT NULL,\n",
    "                                    reading_time INTEGER\n",
    "                                \n",
    "                            );\n",
    "                            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1b73d4ccdc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute sql statement\n",
    "cur.execute(sql_create_table_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test',)]\n"
     ]
    }
   ],
   "source": [
    "# print all tables\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test',)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# drop table\n",
    "helper_functions.print_tables_in_database(cur)\n",
    "helper_functions.drop_table(cur, 'test')\n",
    "helper_functions.print_tables_in_database(cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the extraction process iteratively\n",
    "The HTML request is looking for all articles published on the defined date. By iterating over a list with all dates in a certain time frame, we will get all articles published on TowardsDataScience in this time frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "# returns a list with a entry for each day between sdate and edate\n",
    "days = []\n",
    "\n",
    "sdate = date(2020, 1, 1)   # start date\n",
    "edate = date(2020, 1, 3)   # end date\n",
    "\n",
    "delta = edate - sdate       # as timedelta\n",
    "\n",
    "for i in range(delta.days + 1):\n",
    "    day = sdate + timedelta(days=i)\n",
    "    days.append(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Url: https://towardsdatascience.com/archive/2020/01/01\n"
     ]
    }
   ],
   "source": [
    "for k in range(0,1,1):\n",
    "    year = str(days[k].year)\n",
    "    month = str(days[k].month).zfill(2)\n",
    "    day = str(days[k].day).zfill(2)\n",
    "    \n",
    "    date_published = f'{month}/{day}/{year}'\n",
    "    url = f'https://towardsdatascience.com/archive/{year}/{month}/{day}'\n",
    "    print(\"Url: \" + url)\n",
    "    \n",
    "    df = extract_data(url, date_published)\n",
    "    \n",
    "    # save df to sql\n",
    "    # df.to_sql('stories', con=conn, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://towardsdatascience.com/making-python-programs-blazingly-fast-c1cd79bd1b32?source=collection_archive---------0-----------------------'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"story_url\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read important data directly from story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "figures = soup.find_all('figcaption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Story:\n",
    "    '''\n",
    "        Class to export all required data from a specific story\n",
    "    '''\n",
    "    def __init__(self, url):\n",
    "        self.page = requests.get(url)\n",
    "        self.soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        \n",
    "        \n",
    "        ##########################################################################################################\n",
    "        # Find all figures and figurer captures\n",
    "        ########################################################################################################## \n",
    "        self.figure_captures = []\n",
    "        \n",
    "        figures = self.soup.find_all('figcaption')\n",
    "        \n",
    "        for figure in figures:\n",
    "            capture_modified = str(figure).replace('<figcaption class=\"kl km fy fw fx kn ko bf b bg bh dx\">','')\n",
    "            capture_modified = capture_modified.replace('</figcaption>','')\n",
    "            self.figure_captures.append(capture_modified)\n",
    "            \n",
    "        ##########################################################################################################\n",
    "        # Find all figures and figurer captures\n",
    "        ##########################################################################################################             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regression Algorithms — Image by the author',\n",
       " 'Overview of types of learning — Image by the author',\n",
       " 'Linear Regression: interception term and regression coefficients — Image by the author',\n",
       " 'Global trend models [Fah16, p.512]',\n",
       " 'Polynomial Regression: Sample Model — Image by the author',\n",
       " 'Effects of individual outliers on the linear regression model — Image by the author',\n",
       " 'RANSAC algorithm — Image by the author',\n",
       " 'RANSAC algorithm: Four iterations of the model building process (Min_Samples =2, Threshhold = 20) — Image by the author',\n",
       " 'Decision tree for a simple two-dimensional case with a depth of one — Image by the author',\n",
       " 'Random Forest: Sample Model — Image by the author',\n",
       " 'A-priori Gaussian-Prozess using a Squared Exponential Kernel — Image by the autor (inspired by [Sci18n][Duv14])',\n",
       " 'Squared Exponential Kernel: Influence of Hyperparameters — Image by the author',\n",
       " 'A-priori Gaussian-Prozess using Rational Quadratic Kernel — Image by the autor (inspired by [Sci18n][Duv14])',\n",
       " 'A-priori Gaussian-Prozess using Periodic Kernel — Image by the autor (inspired by [Sci18n][Duv14])',\n",
       " 'Representation of the Gaussian process with the so-called constant kernel and one supporting data point — Image by the author',\n",
       " 'Posterior Gaussian Process with Squared Exponential Kernel and used data points — Image by the author',\n",
       " 'The normal distribution depending on the parameters expected value and variance — Image by the author (inspired by [BB18])',\n",
       " 'Gaussian Process Regression: Sample Model — Image by the author',\n",
       " 'Functionality of the Support Vector Regression —Image by the author (inspired by [Smo04])',\n",
       " 'The soft margin loss setting for a linear SVM — Image by the author (inspired by [Smo04])',\n",
       " 'Overview of the presented regression methods — Image by the author',\n",
       " 'Functionality of cross-validation using the example of 5-fold cross-validation — Image by the author',\n",
       " 'Scoring parameters for the evaluation of regression models — Image by the author',\n",
       " 'Polynomial Regression: Overfitting — Image by the author',\n",
       " 'Choice of polynomial degree using cross-validation — Image by the author',\n",
       " 'Schematic representation of the evaluation of the different regression methods and subsequent model building — Image by the author',\n",
       " 'Overview of the most important hyperparameters of the regression methods — Image by the author']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story = Story(\"https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all the information to the stories given in the medium archive\n",
    "def extract_data(url, date_published):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "    stories_data = []\n",
    "\n",
    "    # find attributes author_url, reading_time, reading_time, responses, story_url for the stories which where published\n",
    "    # for the specified date\n",
    "\n",
    "    for story in stories:\n",
    "        each_story = []\n",
    "\n",
    "        author_box = story.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "        author_url = author_box.find('a')['href']\n",
    "\n",
    "        try:\n",
    "            reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        title = story.find('h3').text if story.find('h3') else '-'\n",
    "        subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "\n",
    "        if story.find('button',\n",
    "                      class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents'):\n",
    "\n",
    "            claps = story.find('button',\n",
    "                               class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents').text\n",
    "\n",
    "        else:\n",
    "            claps = 0\n",
    "\n",
    "        if story.find('a', class_='button button--chromeless u-baseColor--buttonNormal'):\n",
    "\n",
    "            responses = story.find('a', class_='button button--chromeless u-baseColor--buttonNormal').text\n",
    "\n",
    "        else:\n",
    "            responses = '0 responses'\n",
    "\n",
    "        story_url = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')[\n",
    "            'href']\n",
    "\n",
    "        # data cleaning\n",
    "        reading_time = reading_time.split()[0]\n",
    "        responses = responses.split()[0]\n",
    "\n",
    "        story_page = requests.get(story_url)\n",
    "        story_soup = BeautifulSoup(story_page.text, 'html.parser')\n",
    "\n",
    "        sections = story_soup.find_all('section')\n",
    "        story_paragraphs = []\n",
    "        section_titles = []\n",
    "\n",
    "        for section in sections:\n",
    "            paragraphs = section.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                story_paragraphs.append(paragraph.text)\n",
    "\n",
    "            subs = section.find_all('h1')\n",
    "            for sub in subs:\n",
    "                section_titles.append(sub.text)\n",
    "\n",
    "        number_sections = len(section_titles)\n",
    "        number_paragraphs = len(story_paragraphs)\n",
    "\n",
    "        each_story.append(date_published)\n",
    "        each_story.append(title)\n",
    "        each_story.append(subtitle)\n",
    "        each_story.append(claps)\n",
    "        each_story.append(responses)\n",
    "        each_story.append(author_url)\n",
    "        each_story.append(story_url)\n",
    "        each_story.append(reading_time)\n",
    "        each_story.append(number_sections)\n",
    "        each_story.append(section_titles)\n",
    "        each_story.append(number_paragraphs)\n",
    "        each_story.append(story_paragraphs)\n",
    "\n",
    "        stories_data.append(each_story)\n",
    "\n",
    "    # write data to data frame \"df\" and return df\n",
    "    columns = ['date_published', 'title', 'subtitle', 'claps', 'responses',\n",
    "               'author_url', 'story_url', 'reading_time',\n",
    "               'number_sections', 'section_titles',\n",
    "               'number_paragraphs', 'paragraphs']\n",
    "\n",
    "    df = pd.DataFrame(stories_data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regression Algorithms — Image by the author',\n",
       " 'Overview of types of learning — Image by the author',\n",
       " 'Linear Regression: interception term and regression coefficients — Image by the author',\n",
       " 'Global trend models [Fah16, p.512]',\n",
       " 'Polynomial Regression: Sample Model — Image by the author',\n",
       " 'Effects of individual outliers on the linear regression model — Image by the author',\n",
       " 'RANSAC algorithm — Image by the author',\n",
       " 'RANSAC algorithm: Four iterations of the model building process (Min_Samples =2, Threshhold = 20) — Image by the author',\n",
       " 'Decision tree for a simple two-dimensional case with a depth of one — Image by the author',\n",
       " 'Random Forest: Sample Model — Image by the author',\n",
       " 'A-priori Gaussian-Prozess using a Squared Exponential Kernel — Image by the autor (inspired by [Sci18n][Duv14])',\n",
       " 'Squared Exponential Kernel: Influence of Hyperparameters — Image by the author',\n",
       " 'A-priori Gaussian-Prozess using Rational Quadratic Kernel — Image by the autor (inspired by [Sci18n][Duv14])',\n",
       " 'A-priori Gaussian-Prozess using Periodic Kernel — Image by the autor (inspired by [Sci18n][Duv14])',\n",
       " 'Representation of the Gaussian process with the so-called constant kernel and one supporting data point — Image by the author',\n",
       " 'Posterior Gaussian Process with Squared Exponential Kernel and used data points — Image by the author',\n",
       " 'The normal distribution depending on the parameters expected value and variance — Image by the author (inspired by [BB18])',\n",
       " 'Gaussian Process Regression: Sample Model — Image by the author',\n",
       " 'Functionality of the Support Vector Regression —Image by the author (inspired by [Smo04])',\n",
       " 'The soft margin loss setting for a linear SVM — Image by the author (inspired by [Smo04])',\n",
       " 'Overview of the presented regression methods — Image by the author',\n",
       " 'Functionality of cross-validation using the example of 5-fold cross-validation — Image by the author',\n",
       " 'Scoring parameters for the evaluation of regression models — Image by the author',\n",
       " 'Polynomial Regression: Overfitting — Image by the author',\n",
       " 'Choice of polynomial degree using cross-validation — Image by the author',\n",
       " 'Schematic representation of the evaluation of the different regression methods and subsequent model building — Image by the author',\n",
       " 'Overview of the most important hyperparameters of the regression methods — Image by the author']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "\n",
    "def extract_figure_captures(soup):\n",
    "    figures = soup.find_all('figcaption')\n",
    "    \n",
    "    figure_capture = []\n",
    "\n",
    "    for figure in figures:\n",
    "        capture_modified = str(figure).replace('<figcaption class=\"kl km fy fw fx kn ko bf b bg bh dx\">','')\n",
    "        capture_modified = capture_modified.replace('</figcaption>','')\n",
    "        figure_capture.append(capture_modified)\n",
    "\n",
    "    figure_capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1feb0438eeb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"figure\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2172\u001b[0m         \u001b[1;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2173\u001b[0m         raise AttributeError(\n\u001b[1;32m-> 2174\u001b[1;33m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2175\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "class SQLiteConnection:\n",
    "\n",
    "    def __init__(self, db_file):\n",
    "        \"\"\" create a database connection to the SQLite database\n",
    "            specified by db_file\n",
    "        :param db_file: database file\n",
    "        :return: Connection object or None\n",
    "        \"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(db_file)\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "        return conn\n",
    "\n",
    "    def create_table(cur, create_table_sql):\n",
    "        \"\"\" create a table from the create_table_sql statement\n",
    "        :param cur: Defined cursor\n",
    "        :param create_table_sql: a CREATE TABLE statement\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cur.execute(create_table_sql)\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "    def drop_table(cur, table_to_drop):\n",
    "        \"\"\" drop a table\n",
    "        :param conn: Defined cursor\n",
    "        :param table_to_drop: specify the table name you want to drop\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        sql_drop_table = f\"drop table {table_to_drop}\"\n",
    "\n",
    "        try:\n",
    "            cur.execute(sql_drop_table)\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "    def print_tables_in_database(cur):\n",
    "        try:\n",
    "            cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            print(cur.fetchall())\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
