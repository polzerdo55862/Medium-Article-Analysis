{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "This notebook collects the data from different data sources and end points and saves the raw data in a SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following I search for articles published in a certain time frame in the TowardsDataScience archive (https://towardsdatascience.com/archive/year/month/day) \n",
    "\n",
    "References: \n",
    "https://hackernoon.com/how-to-scrape-a-medium-publication-a-python-tutorial-for-beginners-o8u3t69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# function to extract all the information to the stories given in the medium archive\n",
    "def extract_data(url, date_published):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "    stories_data = []\n",
    "    \n",
    "    # find attributes author_url, reading_time, reading_time, responses, story_url for the stories which where published \n",
    "    # for the specified date\n",
    "    \n",
    "    for story in stories:\n",
    "        each_story = []\n",
    "\n",
    "        author_box = story.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "        author_url = author_box.find('a')['href']\n",
    "\n",
    "        try:\n",
    "            reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        title = story.find('h3').text if story.find('h3') else '-'\n",
    "        subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "\n",
    "        if story.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents'):\n",
    "\n",
    "            claps = story.find('button', class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents').text\n",
    "\n",
    "        else:\n",
    "            claps = 0\n",
    "\n",
    "        if story.find('a', class_='button button--chromeless u-baseColor--buttonNormal'):\n",
    "\n",
    "            responses = story.find('a', class_='button button--chromeless u-baseColor--buttonNormal').text\n",
    "\n",
    "        else:\n",
    "            responses = '0 responses'\n",
    "\n",
    "        story_url = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')['href']\n",
    "   \n",
    "        # data cleaning\n",
    "        reading_time = reading_time.split()[0]\n",
    "        responses = responses.split()[0]\n",
    "\n",
    "        story_page = requests.get(story_url)\n",
    "        story_soup = BeautifulSoup(story_page.text, 'html.parser')\n",
    "\n",
    "        sections = story_soup.find_all('section')\n",
    "        story_paragraphs = []\n",
    "        section_titles = []\n",
    "\n",
    "        for section in sections:\n",
    "            paragraphs = section.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                story_paragraphs.append(paragraph.text)\n",
    "\n",
    "            subs = section.find_all('h1')\n",
    "            for sub in subs:\n",
    "                section_titles.append(sub.text)\n",
    "\n",
    "        number_sections = len(section_titles)\n",
    "        number_paragraphs = len(story_paragraphs)\n",
    "\n",
    "        each_story.append(date_published)\n",
    "        each_story.append(title)\n",
    "        each_story.append(subtitle)\n",
    "        each_story.append(claps)\n",
    "        each_story.append(responses)\n",
    "        each_story.append(author_url)\n",
    "        each_story.append(story_url)\n",
    "        each_story.append(reading_time)\n",
    "        each_story.append(number_sections)\n",
    "        each_story.append(section_titles)\n",
    "        each_story.append(number_paragraphs)\n",
    "        each_story.append(story_paragraphs)\n",
    "\n",
    "        stories_data.append(each_story)\n",
    "\n",
    "    # write data to data frame \"df\" and return df\n",
    "    columns = ['date_published', 'title', 'subtitle', 'claps', 'responses', \n",
    "           'author_url', 'story_url', 'reading_time', \n",
    "           'number_sections', 'section_titles', \n",
    "           'number_paragraphs', 'paragraphs']\n",
    "\n",
    "    df = pd.DataFrame(stories_data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://towardsdatascience.com/archive/2020/01/01'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "for story in stories:\n",
    "    story_url = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')['href']\n",
    "    print(story_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results in a SQLite database\n",
    "After each scraping process, save the content of the data frame into a SQLite database. You can find the functions used in the module helper_functions.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import helper_functions\n",
    "\n",
    "# connect to SQLite database medium.db\n",
    "con = helper_functions.create_sqlite_connection(\"medium.db\")\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sql statement to create table \"stories\" if not exist\n",
    "sql_create_table_stories = \"\"\"\n",
    "                                CREATE TABLE IF NOT EXISTS test (\n",
    "                                \n",
    "                                    date_published INTEGER PRIMARY KEY,\n",
    "                                    title TEXT NOT NULL,\n",
    "                                    subtitle TEXT,\n",
    "                                    claps TEXT NOT NULL,\n",
    "                                    responses INTEGER,\n",
    "                                    author_url TEXT NOT NULL,\n",
    "                                    reading_time INTEGER\n",
    "                                \n",
    "                            );\n",
    "                            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute sql statement\n",
    "cur.execute(sql_create_table_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all tables\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop table\n",
    "helper_functions.print_tables_in_database(cur)\n",
    "helper_functions.drop_table(cur, 'test')\n",
    "helper_functions.print_tables_in_database(cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the extraction process iteratively\n",
    "The HTML request is looking for all articles published on the defined date. By iterating over a list with all dates in a certain time frame, we will get all articles published on TowardsDataScience in this time frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "# returns a list with a entry for each day between sdate and edate\n",
    "days = []\n",
    "\n",
    "sdate = date(2020, 1, 1)   # start date\n",
    "edate = date(2020, 1, 3)   # end date\n",
    "\n",
    "delta = edate - sdate       # as timedelta\n",
    "\n",
    "for i in range(delta.days + 1):\n",
    "    day = sdate + timedelta(days=i)\n",
    "    days.append(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,1,1):\n",
    "    year = str(days[k].year)\n",
    "    month = str(days[k].month).zfill(2)\n",
    "    day = str(days[k].day).zfill(2)\n",
    "    \n",
    "    date_published = f'{month}/{day}/{year}'\n",
    "    url = f'https://towardsdatascience.com/archive/{year}/{month}/{day}'\n",
    "    print(\"Url: \" + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions\n",
    "\n",
    "story = helper_functions.Story(year, month, day)\n",
    "story.stories.date_published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story.stories.date_published"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read important data directly from story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "figures = soup.find_all('figcaption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Story:\n",
    "    '''\n",
    "        Class to export all required data from a specific story\n",
    "    '''\n",
    "    def __init__(self, url):\n",
    "        self.page = requests.get(url)\n",
    "        self.soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        \n",
    "        \n",
    "        ##########################################################################################################\n",
    "        # Find all figures and figurer captures\n",
    "        ########################################################################################################## \n",
    "        self.figure_captures = []\n",
    "        \n",
    "        figures = self.soup.find_all('figcaption')\n",
    "        \n",
    "        for figure in figures:\n",
    "            capture_modified = str(figure).replace('<figcaption class=\"kl km fy fw fx kn ko bf b bg bh dx\">','')\n",
    "            capture_modified = capture_modified.replace('</figcaption>','')\n",
    "            self.figure_captures.append(capture_modified)\n",
    "            \n",
    "        ##########################################################################################################\n",
    "        # Find all figures and figurer captures\n",
    "        ##########################################################################################################             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = Story(\"https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all the information to the stories given in the medium archive\n",
    "def extract_data(url, date_published):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    stories = soup.find_all('div', class_='streamItem streamItem--postPreview js-streamItem')\n",
    "    stories_data = []\n",
    "\n",
    "    # find attributes author_url, reading_time, reading_time, responses, story_url for the stories which where published\n",
    "    # for the specified date\n",
    "\n",
    "    for story in stories:\n",
    "        each_story = []\n",
    "\n",
    "        author_box = story.find('div', class_='postMetaInline u-floatLeft u-sm-maxWidthFullWidth')\n",
    "        author_url = author_box.find('a')['href']\n",
    "\n",
    "        try:\n",
    "            reading_time = author_box.find('span', class_='readingTime')['title']\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        title = story.find('h3').text if story.find('h3') else '-'\n",
    "        subtitle = story.find('h4').text if story.find('h4') else '-'\n",
    "\n",
    "        if story.find('button',\n",
    "                      class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents'):\n",
    "\n",
    "            claps = story.find('button',\n",
    "                               class_='button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-disablePointerEvents').text\n",
    "\n",
    "        else:\n",
    "            claps = 0\n",
    "\n",
    "        if story.find('a', class_='button button--chromeless u-baseColor--buttonNormal'):\n",
    "\n",
    "            responses = story.find('a', class_='button button--chromeless u-baseColor--buttonNormal').text\n",
    "\n",
    "        else:\n",
    "            responses = '0 responses'\n",
    "\n",
    "        story_url = story.find('a', class_='button button--smaller button--chromeless u-baseColor--buttonNormal')[\n",
    "            'href']\n",
    "\n",
    "        # data cleaning\n",
    "        reading_time = reading_time.split()[0]\n",
    "        responses = responses.split()[0]\n",
    "\n",
    "        story_page = requests.get(story_url)\n",
    "        story_soup = BeautifulSoup(story_page.text, 'html.parser')\n",
    "\n",
    "        sections = story_soup.find_all('section')\n",
    "        story_paragraphs = []\n",
    "        section_titles = []\n",
    "\n",
    "        for section in sections:\n",
    "            paragraphs = section.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                story_paragraphs.append(paragraph.text)\n",
    "\n",
    "            subs = section.find_all('h1')\n",
    "            for sub in subs:\n",
    "                section_titles.append(sub.text)\n",
    "\n",
    "        number_sections = len(section_titles)\n",
    "        number_paragraphs = len(story_paragraphs)\n",
    "\n",
    "        each_story.append(date_published)\n",
    "        each_story.append(title)\n",
    "        each_story.append(subtitle)\n",
    "        each_story.append(claps)\n",
    "        each_story.append(responses)\n",
    "        each_story.append(author_url)\n",
    "        each_story.append(story_url)\n",
    "        each_story.append(reading_time)\n",
    "        each_story.append(number_sections)\n",
    "        each_story.append(section_titles)\n",
    "        each_story.append(number_paragraphs)\n",
    "        each_story.append(story_paragraphs)\n",
    "\n",
    "        stories_data.append(each_story)\n",
    "\n",
    "    # write data to data frame \"df\" and return df\n",
    "    columns = ['date_published', 'title', 'subtitle', 'claps', 'responses',\n",
    "               'author_url', 'story_url', 'reading_time',\n",
    "               'number_sections', 'section_titles',\n",
    "               'number_paragraphs', 'paragraphs']\n",
    "\n",
    "    df = pd.DataFrame(stories_data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "\n",
    "def extract_figure_captures(soup):\n",
    "    figures = soup.find_all('figcaption')\n",
    "    \n",
    "    figure_capture = []\n",
    "\n",
    "    for figure in figures:\n",
    "        capture_modified = str(figure).replace('<figcaption class=\"kl km fy fw fx kn ko bf b bg bh dx\">','')\n",
    "        capture_modified = capture_modified.replace('</figcaption>','')\n",
    "        figure_capture.append(capture_modified)\n",
    "\n",
    "    figure_capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLiteConnection:\n",
    "\n",
    "    def __init__(self, db_file):\n",
    "        \"\"\" create a database connection to the SQLite database\n",
    "            specified by db_file\n",
    "        :param db_file: database file\n",
    "        :return: Connection object or None\n",
    "        \"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(db_file)\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "        return conn\n",
    "\n",
    "    def create_table(cur, create_table_sql):\n",
    "        \"\"\" create a table from the create_table_sql statement\n",
    "        :param cur: Defined cursor\n",
    "        :param create_table_sql: a CREATE TABLE statement\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cur.execute(create_table_sql)\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "    def drop_table(cur, table_to_drop):\n",
    "        \"\"\" drop a table\n",
    "        :param conn: Defined cursor\n",
    "        :param table_to_drop: specify the table name you want to drop\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        sql_drop_table = f\"drop table {table_to_drop}\"\n",
    "\n",
    "        try:\n",
    "            cur.execute(sql_drop_table)\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "    def print_tables_in_database(cur):\n",
    "        try:\n",
    "            cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            print(cur.fetchall())\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains some rep\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import data_model\n",
    "\n",
    "######################################################################################################################\n",
    "# Functions to interact with the SQLite database\n",
    "######################################################################################################################\n",
    "\n",
    "class SQLiteConnection:\n",
    "\n",
    "    def __init__(self, db_file):\n",
    "        \"\"\" create a database connection to the SQLite database\n",
    "            specified by db_file\n",
    "        :param db_file: database file\n",
    "        :return: Connection object or None\n",
    "        \"\"\"\n",
    "        self.conn = None\n",
    "        self.cur = None\n",
    "\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(db_file)\n",
    "            self.cur = self.conn.cursor()\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "        # (if not exist) create table stories\n",
    "        try:\n",
    "            self.cur.execute(data_model.sql_create_table_stories)\n",
    "            return \"Tables Stories was created or is existing\"\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "\n",
    "\n",
    "    def drop_table(self, table_to_drop):\n",
    "        \"\"\"\n",
    "        drop a table\n",
    "            :param table_to_drop: String, Name of the table that should be dropped\n",
    "        \"\"\"\n",
    "\n",
    "        sql_drop_table = f\"drop table {table_to_drop}\"\n",
    "\n",
    "        try:\n",
    "            self.cur.execute(sql_drop_table)\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])\n",
    "\n",
    "    def print_tables(self):\n",
    "        try:\n",
    "            self.cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            print(self.cur.fetchall())\n",
    "        except:\n",
    "            print(\"Error:\", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions\n",
    "sqlite = helper_functions.SQLiteConnection(\"medium.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2020\"\n",
    "month = \"02\"\n",
    "day = \"02\"\n",
    "stories = helper_functions.Story(sqlite.conn, year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table stories has no column named index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d1f3300975b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stories\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2786\u001b[0m             \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2787\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2788\u001b[1;33m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2789\u001b[0m         )\n\u001b[0;32m   2790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m         \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     )\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   1842\u001b[0m         )\n\u001b[0;32m   1843\u001b[0m         \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m         \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhas_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, chunksize, method)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m                 \u001b[0mexec_insert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m     def _query_iterator(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[1;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[0;32m   1565\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_insert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1567\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert_statement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_insert_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: table stories has no column named index"
     ]
    }
   ],
   "source": [
    "stories.stories.to_sql(\"stories\", sqlite.conn, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('test',), ('stories',)]\n"
     ]
    }
   ],
   "source": [
    "sqlite.print_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "cnx = mysql.connector.connect(user='scott', password='password',\n",
    "                              host='127.0.0.1',\n",
    "                              database='employees')\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions\n",
    "sqlite = helper_functions.SQLiteConnection(\"medium.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
